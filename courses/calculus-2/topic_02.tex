\chapter{Differentiation of $\Rtn\longrightarrow\R^m$ functions}
\thispagestyle{noheaders}

In this chapter we are extending the notion of derivative of a function and differentiability to functions of several
real variables. For this we'll need some linear algebra concepts about linear maps, knowing that they are continuous at
every point.

\section{Differentiable functions in $\Rtn$}
Let's begin by extending the notion of differentiability to $n$ dimensions and introducing the concept of total derivative.

\begin{defn}[Differentiable function]\label{def:differentiable-function}
    Let $S\subseteq\Rtn$ be an \nref{def:open-set}. Then a function $\vappl{f}{S\subset\Rtn}{\R^m}$ is (totally) differentiable
    at a point $\vec{x_0}\in S$ if there exists a linear transformation $\appl{\dd\vec{f_{x_0}}}{\Rtn}{\R^m}$ such that
    \begin{equation}
        \lim_{\vec{x}\to\vec{x_0}}\frac{\vnorm{\vec{f}(\vec{x}) - \vec{f}(\vec{x_0}) - \dd\vec{f_{x_0}}(\vec{x} 
        - \vec{x_0})}}{\vnorm{\vec{x} - \vec{x_0}}} = 0.
    \end{equation}
\end{defn}

\begin{remark}
    The linear transformation $\dd\vec{f_{x_0}}$ is unique and it's called the \textit{(total) derivative} or 
    \textit{(total) differential} of $\vec{f}$ at $\vec{x_0}$, which can also be denoted by $\textrm{D}\ \vec{f}(\vec{x_0})$.
\end{remark}

\begin{note}
    A function is (totally) differentiable if its differential exists at every point in its domain.
\end{note}

Conceptually, the definition of the differential expresses the idea that $\dd\vec{f_{x_0}}$ is the best linear approximation
to $\vec{f}$ at the point $\vec{x_0}$. This can be made precise by quantifying the error in the linear approximation
determined by $\dd\vec{f_{x_0}}$ as
\begin{equation}
    \vec{f}(\vec{x_0} + \vec{h}) = \vec{f}(\vec{x_0}) + \dd\vec{f_{x_0}}(\vec{h}) + o(\vnorm{\vec{h}}),
\end{equation}
where $o(\vnorm{\vec{h}})$ is the error approximation, which in asymptotic notation indicates that $o(\vnorm{\vec{h}})$ is
much smaller than $\vnorm{\vec{h}}$ as $\vec{h}\longrightarrow\vec{0}$. So, the differential is the unique linear transformation 
which minimizes this error, thus being the best linear approximation to $\vec{f}$.

\begin{prop}
    If $\vec{f}$ is differentiable at a point $\vec{x_0}\in\dom{\vec{f}}$, then $\vec{f}$ is continuous at 
    the point $\vec{x_0}$.
\end{prop}

\begin{prop}
    A function $\vappl{f}{\Rtn}{\R^m}$ is differentiable $\iff$ each of its components $\appl{f_i}{\Rtn}{\R}$ is
    differentiable.
\end{prop}

We'll begin by generalizing the concept of derivative seen for functions of a single real variable to functions with
$n$ real variables.

\begin{defn}[Partial derivative]\label{def:partial-derivative}
    Let $\vappl{f}{\Rtn}{\R^m}$ such that $\vec{x}\longmapsto \vec{f}(\vec{x})$ and let $\vec{x_0}\in\dom{\vec{f}}$. 
    The partial derivative of $\vec{f}$ with respect to the variable $x_i$ is
    \begin{equation}
        \partialdv{}{x_i} \vec{f}(\vec{x_0})\bydef\lim_{h\to 0}\frac{\vec{f}(\vec{x_0} + h\vec{e_i}) - \vec{f}(\vec{x_0})}{h},
    \end{equation}
    where $\vec{e_i}$ is the $i$-th vector of the canonical basis of $\Rtn$.
\end{defn}

\begin{remark}
    To compute a \nref{def:partial-derivative} the function is differentiated with respect to the variable $x_i$ while taking 
    the rest as constants.
\end{remark}

\begin{note}
    If all the partial derivatives exist at each point of $\dom{\vec{f}}$ and they are continuous, the function $\vec{f}$ is
    said to be of class $\mathcal{C}^1$, written as $\vec{f}\in\mathcal{C}^1$.
\end{note}

Unlike partial derivatives, the total derivative or differential approximates the function with respect to all of its
arguments, not just a single one. So, in many situations taking the differential $\dd\vec{f_{x_0}}$ is the same as 
considering all partial derivatives simultaneously.

\begin{theorem}
    Let $\appl{f}{S\subset\Rtn}{\R}$ with $S$ open. If $f\in\mathcal{C}^1\implies$ $f$ is differentiable in $S$.
\end{theorem}

Let's study now the different forms the differential can take depending on how is the function constructed. If our
function is a \nref{def:scalar-function}, the differential will be a vector containing the partial derivatives of the 
function. This vector is known as the gradient.

\begin{defn}[Gradient]\label{def:gradient}
    Let $\appl{f}{S\subset\Rtn}{\R}$ be a \nref{def:scalar-function}, which we'll refer to as \textit{scalar field},
    differentiable at each point of $S$. The gradient of $f$ is the \nref{def:vector-function} $\appl{\grad f}{S}{\Rtn}$
    defined as
    \begin{equation}
        \grad f(\vec{x})\bydef\rvec{\displaystyle\partialdv{f}{x_1}(\vec{x_0}) & \cdots & \displaystyle
        \partialdv{f}{x_n}(\vec{x_0})} = \partialdv{f}{x_1}\vec{e1} + \cdots + \partialdv{f}{x_n}\vec{e_i}.
    \end{equation}
\end{defn}

The gradient $\grad f$ allows us to express the derivative of $f$ in any direction simply as an inner product, yielding 
the notion of \textit{directional derivative}.

\begin{defn}[Directional derivative]\label{def:directional-derivative}
    Let $\appl{f}{S\subset\Rtn}{\R}$ differentiable at $\vec{x_0}\in S$ and let $\uvec{v}\in\Rtn$ be a unit vector. The
    directional derivative of $f$ at $\vec{x_0}$ in the direction of $\uvec{v}$, denoted by $\textrm{D}_\vec{v}f(\vec{x_0})$,
    is the derivative evaluated at $0$ of the function given by $t\in\R\st t\longmapsto f(\vec{x_0} + t\uvec{v})$. That is,
    \begin{equation}
        \textrm{D}_\vec{v}f(\vec{x_0})\bydef\frac{\dd}{\dd t} f(\vec{x_0} + ลง\uvec{v})\big|_{t=0}.
    \end{equation}
\end{defn}


\begin{defn}[Jacobian matrix]\label{def:jacobian-matrix}
    Let $\vappl{f}{\Rtn}{\R^m}$ such that each of its first-order partial derivatives exist on $\Rtn$, and let $\vec{x_0}\in
    \dom{\vec{f}}$, the Jacobian matrix of $\vec{f}$ at $\vec{x_0}$, $\vec{J}(\vec{f})\in\R^{m\times n}$ is defined as
    follows.
    \begin{equation}
    \vec{J}(\vec{f})\bydef\rvec{\displaystyle\partialdv{\vec{f}}{x_1} & \cdots & \displaystyle\partialdv{\vec{f}}{x_n}}\bydef
    \cvec{\grad^T f_1 \\ \vdots \\ \grad^T f_m} = \begin{bmatrix}
        \displaystyle\partialdv{f_1}{x_1} & \cdots & \displaystyle\partialdv{f_1}{x_n} \\
        \vdots & \ddots & \vdots \\
        \displaystyle\partialdv{f_m}{x_1} & \cdots & \displaystyle\partialdv{f_m}{x_n}
    \end{bmatrix},
\end{equation}
where $\grad^T f_i$ is the transpose of the \nref{def:gradient} of the $i$-th component.
\end{defn}

\begin{note}
    The \nref{def:jacobian-matrix} of a function is usually referred to as simply the Jacobian of the function.
\end{note}

\begin{remark} 
    The differential of a \nref{def:vector-function} is given by the \nref{def:jacobian-matrix} corresponding to that 
    function.
\end{remark}


\section{Properties of differentiable functions}

\section{Higher-order partial derivatives and differentials}

\section{Directional derivatives}

\section{Divergence and curl. Vector fields}

\section{Taylor's expansion in $\Rtn$}

\section{Maxima and minima. Lagrange multipliers for constrained extrema}

\section{Inverse function theorem}

\section{Implicit functions}

\section{Differentiable manifolds}

