\chapter{Vector spaces}

\begin{defi}[Vector space]
    A vector space over a field $F$ is a set $V$ together with two operations, $\appl{+}{V\times V}{V}$
    and $\appl{\cdot}{F\times V}{V}$, such that $\left( V, + \right) $ is an abelian group, and that for any
    $\vec{v_1}$, $\vec{v_2}\in V$, $\alpha_1$, $\alpha_2\in F$ the following properties are verified:
    \begin{align*}
        & 1\cdot \vec{v_1} = \vec{v_1}, \quad\quad\quad\quad & \left( \alpha_1 + \alpha_2 \right) \cdot \vec{v_1} = \alpha_1\vec{v_1} + \alpha_2\vec{v_1}, \\
        & \alpha_1\cdot\left( \alpha_2\cdot \vec{v_1} \right) = \left( \alpha_1\alpha_2 \right) \cdot
        \vec{v_1}, \quad\quad\quad\quad &  \alpha_1\cdot\left( \vec{v_1 + v_2} \right) = \alpha_1\vec{v_1} + \alpha_1\vec{v_2}.
    \end{align*}

    \hide{
    \begin{itemize}
        \item[] $1\cdot \vec{v_1} = \vec{v_1}$,
        \item[] $\alpha_1\cdot\left( \alpha_2\cdot \vec{v_1} \right) = \left( \alpha_1\alpha_2 \right) \cdot
            \vec{v_1}$,
        \hide{
        \item $\left( \alpha_1 + \alpha_2 \right) \cdot \vec{v_1} = \alpha_1\vec{v_1} + \alpha_2\vec{v_1}$,
        \item $\alpha_1\cdot\left( \vec{v_1 + v_2} \right) = \alpha_1\vec{v_1} + \alpha_1\vec{v_2}$.
        }
    }
    \end{itemize}
    \hide{
    A vector space over a field $F$ is a set $V$ together with two operations that satisfy the eight axioms
    listed below.
    \begin{itemize}
        \item The first operation, called \textbf{vector addition} $\appl{+}{V\times V}{V}$, takes any two
            vectors $\vec{v}$ and $\vec{w}$ and assigns to them a third vector written $\vec{v} + \vec{w}$,
            and called the sum of these two vectors. In this case, the resultant vector is also an element
            of the set $V$.
        \item The second operation, called \textbf{scalar multiplication} $\appl{\cdot}{F\times V}{V}$ takes
            any scalar $a$ and any vector $\vec{v}$ and gives another vector $a\vec{v}$. Similarly, the
            vector $a\vec{v}$ is an element of the set $V$.
    \end{itemize}
    }
\end{defi}

\hide{
\begin{remark}
    Scalar multiplication is not to be confused with the scalar product between two vectors.
\end{remark}
}

\begin{notation}
Elements in $F$ are usually called \textit{scalars} to differentiate them from the ones in $V$, that are
\textit{vectors}.
\end{notation}

\begin{example}
    $\Rtn$, with $n\in\Z^+$, is a vector space over $\R$ defined by
    \begin{equation}
        \Rtn\bydef \underbrace{\R\times\ldots\times\R}_{n \textrm{ times}} = \{\left( \alpha_1, \ldots,
    \alpha_n \right)\st \alpha_i\in\R\}
    \end{equation}
    with addition and scalar multiplication
    \begin{align}
        &\left( \alpha_1, \ldots, \alpha_n \right) + \left( \beta_1, \ldots, \beta_n \right) = \left( \alpha_1          + \beta_1, \ldots, \alpha_n + \beta_n\right) \\
        &\lambda\left( \alpha_1, \ldots, \alpha_n \right) = \left( \lambda\alpha_1, \ldots, \lambda\alpha_n
        \right).
    \end{align}
    This is the main vector space in which we will work in this topic.
\end{example}

From any field $F$, a new vector space can be defined over $F$ in the same manner as the previous example.

\begin{example}
    $m\times n$ matrices with coefficients in a field $F$, form the vector space
    \begin{equation}
        \mathcal{M}_{m\times n}\left( F \right) = \left\{
            \begin{bmatrix}
                a_{11} & a_{12} & \cdots & a_{1n} \\
                a_{21} & a_{22} & \cdots & a_{2n} \\
                \vdots & \vdots & \ddots & \vdots \\
                a_{m1} & a_{m2} & \cdots & a_{mn}
            \end{bmatrix} \st a_{ij}\in F,\ 1 \leq i \leq m,\ 1\leq j\leq n
        \right\}.
    \end{equation}
\end{example}
%In this topic we will work in $\R^n = \underbrace{\R\times\ldots\times\R}_{n \textrm{ times}}$, which a vector space over \R.

\begin{example}
    The solutions $\vec{x} = \left( x_1, \ldots, x_n \right) $ of the homogeneous system $A\vec{x} = B$
    with $A = \left( a_{ij} \right) \in F$ form a vector space over $F$.
\end{example}

\section{Vector subspaces}
\begin{defi}[Vector subspace] \label{def:vector-subspace}
    Let $V$ be a vector space over a field $F$. A nonempty subset $W\subset V$ defines a subspace of $V
    \iff \forall \lambda, \mu\in F$ and $\vec{w_1}, \vec{w_2}\in W\implies \lambda\vec{w_1} +
        \mu\vec{w_2}\in W$.
\end{defi}

To check if some subset $W$ of a vector space $V$ is a subspace of $V$ it's enough to see if $W$ accomplish
definition \ref{def:vector-subspace}. However, in many cases is easier to check that the following properties
are verified:
\begin{equation}
    \vec{w_1}, \vec{w_2}\in W \quad\implies\quad \vec{w_1} + \vec{w_2}\in W,
\end{equation}
\begin{equation} \label{prop:vector-subspace-2}
    \vec{w}\in W,\ \lambda\in F\quad\implies\quad \lambda\vec{w}\in W.
\end{equation}

\begin{example}
    $W = \{\left( x, y, 0 \right)\in \R^3\st x, y\in\R\} $ is a vector subspace of $\R^3$, while $U = \{\left( x, y, 1 \right) \in\R^3\st x, y\in\R\} $ is not. Note that, for instance, $\left( 1, 1, 1 \right)\in U$ but
    $2\cdot\left( 1, 1, 1 \right) \not\in U$, which contradicts property \ref{prop:vector-subspace-2}.
\end{example}

\begin{example}
    $W = \{\left( x, y, z \right) \in\R^3\st x + y + z = 0, 2x - 3y - z = 0\} $ is a vector subspace of $\R^3$,
    but also of $V = \{\left( x, y, z \right) \in\R^3\st x + y + z = 0\} $.
\end{example}

In conclusion, whenever we set linear and homogeneous conditions in $\Rtn$, a subspace is obtained. This is
closely related to the description of a vector space as the solutions of an homogeneous system.

\begin{defi}[Linear combination]
    Let $\vec{v_1}$, $\ldots$, $\vec{v_n}\in V$ be vectors. A linear combination of $\vec{v_1}$, $\ldots$,
    $\vec{v_n}$ is an expression of the form $\alpha_1\vec{v_1} + \ldots + \alpha_n\vec{v_n}$, where
    $\alpha_1$, $\ldots$, $\alpha_n\in F$.
\end{defi}

\begin{example}
    In $\R^2$ every vector $\vec{v} = \left(x, y\right)$ is a linear combination of $\vec{v_1} =
    \left( 1, 0 \right) $ and $\vec{v_2} = \left( 0, 1 \right) $ since $\vec{v} = x\vec{v_1} + y\vec{v_2}$.
\end{example}

\begin{prop}
    In a vector space, the identity element for vector addition, $\vec{0} = \left( 0, \ldots, 0 \right) $, is
    a linear combination of any vector $\vec{v}$, since $\vec{0} = 0\cdot\vec{v}$.
\end{prop}

\begin{defi}[Subspace generated by a set]
    Let $C$ be a subset of a vector space. The vector subspace generated by $C$, denoted by $\gen{C}$ or
    $\mathcal{L}\left( C \right) $, is the set of all the possible vector linear combinations of $C$.
\end{defi}

\begin{prop}
    If $C\subseteq V$, $\gen{C}$ is a vector subspace of $V$.
\end{prop}

\begin{example}
    In $\R^2$, $\gen{\left( 1, 0 \right) , \left( 0, 1 \right) } = \{\alpha_1\left( 1, 0 \right) +
    \alpha_2\left( 0, 1 \right) \} = \R^2 $, since we have already seen in a previous example that every
    vector of $\R^2$ is a linear combination of $\left( 1, 0 \right) $ and $\left( 0, 1 \right) $.
\end{example}

\begin{example}
    Let $C = \{\left( 1, 1, 2 \right), \left( 0, 2, -1 \right), \left( 1, 3, 1 \right) \subset\R^3\} $, then
    \begin{align}
        \gen{C} &= \{\lambda\left( 1, 1, 2 \right) + \mu\left( 0, 2, -1 \right) + \nu\left( 1, 3, 1 \right)\}
             \\ &= \{\left( \lambda + \nu, \lambda + 2\mu + 3\nu, 2\lambda - \mu + \nu \right) \st\lambda,
              \mu, \nu\in\R\},
    \end{align}
    since $\left( 1, 3, 1 \right) $ is a linear combination of $\left( 1, 1, 2 \right) $ and $\left( 0, 2, -1
    \right) $, more concisely
    \begin{equation}
        \left( 1, 3, 1 \right) = \left( 1, 1, 2 \right) + \left( 0, 2, -1 \right),
    \end{equation}
    the vector $\left( 1, 3, 1 \right) $ can be omitted; i.e., $\gen{C} = \gen{\left( 1, 1, 2 \right), \left(
    0, 2, -1\right) }$.
\end{example}

The previous example suggest defining the concept of some dependence between vectors in a set.

\begin{defi}[Linear independence] \label{defi:linear-independence}
    A set of vectors $\{\vec{v_1}, \ldots, \vec{v_n}\} $ from a vector space $V$ is \textbf{linearly
    independent} $\iff$ the equation $\alpha_1\vec{v_1} + \ldots + \alpha_n\vec{v_n} = \vec{0}$ can only
    be satisfied by $\alpha_i = 0$ for $i = 1, \ldots, n$. Otherwise, the set of vectors is
    \textbf{linearly dependent}.
\end{defi}

\begin{remark}
    Definition \ref{defi:linear-independence} implies that no vector in a set of linear independent vectors can be written as a linear combination of the remaining vectors in the set. In other words, even more concisely, a set of vectors is linear independent $\iff\vec{0}$ can be represented as a linear combination of its vectors in a unique way.
\end{remark}

\begin{note}
    Many times it's said that several vectors are linearly independent (or dependent), meaning that they
    form a linearly independent (or dependent) set, in this case we take as finite subset the one formed by
    themselves.
\end{note}

\begin{example}
    Vectors $\left( 1, 1, 0 \right)$, $\left( 2, 1, 1 \right)$, $\left( 5, 3, 2 \right)\in\R^3$ are not
    linearly independent. Writing the linear combination
    \begin{equation}
        \alpha_1\left( 1, 1, 0 \right) + \alpha_2\left( 2, 1, 1 \right) + \alpha_3\left( 5, 3, 2 \right) =
        \left( 0, 0, 0 \right)
    \end{equation}
    we reach the system
    \begin{equation}
    \sysdelim.\}\systeme{\alpha_1 + 2\alpha_2 + 5\alpha_3 = 0,\alpha_1 + \alpha_2 + 3\alpha_3 = 0,\alpha_2 + 2\alpha_3 = 0}
        \quad\implies\quad
            \alpha_2 &= -2\alpha_3,\
            \alpha_1 &= -\alpha_3
    \end{equation}
    and since solutions to this system depends on a parameter there are infinite solutions. Therefore, the
    vectors are linearly dependent.
\end{example}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% BEGIN TRASH

\hide{
\begin{defi}[Linearly independent vectors]
    A set of vectors $\vec{v_1}$, $\ldots$, $\vec{v_i}\in\R^n$ are \textbf{linearly independent} if the
    only way to write the null vector $\vec{0} = \left( 0, \ldots, 0 \right) $ as a linear combination of
    $\vec{v_1}$, $\ldots$, $\vec{v_i}$ is by writing $0\cdot\vec{v_1} + \ldots + 0\cdot\vec{v_i} = \vec{0}$.
    Otherwise, the vectors are \textbf{linearly dependent}.
\end{defi}

\newpage

\begin{defi}[Linearly independent vectors]
    Let $\vec{v_1}$, $\ldots$, $\vec{v_i}\in\R^n$ be vectors. $\vec{v_1}$, $\ldots$, $\vec{v_i}$ are linearly
    independent $\iff$ any of the vectors cannot be written as a linear combination of the others. Otherwise,
    vectors $\vec{v_1}$, $\ldots$, $\vec{v_i}$ are linearly dependent.
\end{defi}
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% END TRASH

\begin{prop}
    Any set of vectors containing the null vector, $\vec{0} = \left( 0, \ldots, 0 \right) $ is always
    linearly dependent.
\end{prop}

\begin{prop}
    In general, a non null vector $\vec{v}\in\R^n$ is linearly independent.
\end{prop}

\begin{proof}
The only solution to the equation $\alpha\vec{v} = \vec{0}$, $\alpha\in\R$ is $\alpha = 0$.
Suppose there is another solution with $\alpha\neq 0$. This implies $\exists \invers{\alpha}\in\R$, which
multiplied in both sides of the equation yields
\begin{equation}
    \alpha\vec{v} = \vec{0}\quad \iff\quad \invers{\alpha}\alpha\vec{v} = \invers{\alpha}\vec{0} \quad
        \iff\quad \vec{v} = \vec{0},
\end{equation}
and this is a contradiction. Then, if $\vec{v}\neq \vec{0}$, vector $\vec{v}$ is always linearly independent.
\end{proof}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% start another hide

\hide{
\begin{example}
To check if some vectors, let's say $\vec{v_1} = \left( 1, 2, -1 \right)$,
$\vec{v_2} = \left( 1, 0, 1\right) $ and $\vec{v_3} = \left( 2, 3, 2 \right)\in\R^3$, are linearly independent means finding non-trivial solutions to the equation
\begin{equation}
    \alpha_1\vec{v_1} + \alpha_2\vec{v_2} + \alpha_3\vec{v_3} = \vec{0},
\end{equation}
\begin{equation}
    \alpha_1\left( 1, 2, -1 \right) + \alpha_2\left( 1, 0, 1 \right) + \alpha_3\left( 2, 3, 2 \right) =
    \left( 0, 0, 0 \right).
\end{equation}
One can get these non-trivial solutions by solving the following homogeneous linear system of equations using
Gaussian elimination.
\begin{equation}
    \begin{cases}
        \alpha_1 + \alpha_2 + 2\alpha_3 = 0 \\
        2\alpha_2 + 0\alpha_2 + 3\alpha_3 = 0 \\
        -\alpha_1 + \alpha_2 + 2\alpha_3 = 0
    \end{cases}
    \quad\quad\quad
    A =
    \begin{bmatrix}
        1 & 1 & 2 \\
        -2 & 0 & 3  \\
        -1 & 1 & 2
    \end{bmatrix}
\end{equation}
\begin{equation}
    A^{*} =
    \begin{bmatrix}
        1 & 1 & 2 & 0 \\
        -2 & 0 & 3 & 0 \\
        -1 & 1 & 2 & 0
    \end{bmatrix}
    \sim
    \begin{bmatrix}
        1 & 1 & 2 & 0 \\
        0 & -2 & -3 & 0 \\
        0 & 0 & 0 & 0
    \end{bmatrix}
\end{equation}

\noindent Since $\rank{A} = \rank{A^*} = 2 < \textrm{ number of variables}$, by Rouché-Fröbenius theorem, there are
infinitely many solutions $\implies$ vectors $\vec{v_1}$, $\vec{v_2}$ and $\vec{v_3}$ are linearly dependent.
}
}

\hide{
\begin{prop}
    If a set of vectors $\vec{v_1}$, $\ldots$, $\vec{v_i}\in\R^n$ is linearly dependent then one of the
    vectors can be written as a linear combination of the others.
\end{prop}

\begin{proof}
Since $\vec{v_1}$, $\ldots$, $\vec{v_i}$ are linearly dependent there are scalars $\alpha_1$,
\ldots, $\alpha_i\in\R$ such that $\alpha_1\vec{v_1} + \ldots + \alpha_i\vec{v_i} = \vec{0}$ and at least one
of the $\alpha_i$-s is non zero. Suppose just to ease the notation that $\alpha_1\neq 0\implies \vec{v_1} =
\invers{\alpha_1}\left( -\alpha_2\vec{v_2} + \ldots + \left( -\alpha_i \right) \vec{v_i} \right) $.
\end{proof}
}

\hide{
\begin{coro}
    All vector spaces are equipped with at least two subspaces: the singleton set with the zero vector and
    the vector space itself. These are called \textbf{trivial subspaces} of the vector space.
\end{coro}

\noindent\textbf{Example.} Let $W = \{\left( x, y \right) \in\R^2 \st x + y = 0\}\subseteq \R^2 $. Is $W$ a
vector subspace of $\R^2$?
\begin{itemize}
    \item We first check if $W\neq \O$. $\left( 1, -1 \right) \in W\implies W\neq \O$.
    \item If $\vec{w_1} = \left( x, y \right) ^t$, $\vec{w_2} = \left( x', y' \right) ^t\in W\implies
        \vec{w_1} + \vec{w_2}\in W$? Suppose $\left( x + x' \right) + \left( y + y' \right) = \left( x + y \right) +
        \left( x' + y' \right) \underbrace{=}_{\vec{w_1}, \vec{w_2}\in W} 0 \implies \vec{w_1} + \vec{w_2}\in W$.
    \item If $\vec{w} = \left( x, y \right) ^t\in W$ and $\alpha\in\R\implies \alpha\vec{w}\in W$? Suppose $\alpha\vec{w} = \left(
        \alpha x, \alpha y\right) ^t \in W$. $\alpha x + \alpha y = \alpha\left( x + y \right) \underbrace{=}_{\vec{w}\in W} \alpha 0 = 0\implies \alpha\vec{w} \in W \implies W$ is a vector subspace of $\R^2$.
\end{itemize}
}

\hide{
\noindent\textbf{Example.} Is $W = \{\left( x, y \right) \in\R^2\st xy = 0\} $ a vector subspace of $\R^2$?
\begin{enumerate}
    \item Is $W\neq \O$? $\left( 1, 0 \right) \in W\implies W\neq \O$.
    \item If $\vec{w_1}$, $\vec{w_2}\in W\overset{?}{\implies} \vec{w_1} + \vec{w_2}\in W$.
        \begin{equation}
            \begin{cases}
                \textrm{let } \vec{w_1} = \left( 0, 1 \right)\in W \\
                \textrm{let } \vec{w_2} = \left( 1, 0 \right)\in W
            \end{cases}
            \quad\implies\quad \vec{w_1} + \vec{w_2} = \left( 1, 1 \right) \not\in W\quad\implies
        \end{equation}
        $\implies\quad W$ is not a vector subspace of $\R^2$.
    \item If 2 had been held, we would have checked if $\vec{w}\in W$ and $\alpha \in\R\overset{?}{\implies}
        \alpha\vec{w}\in W$. This can be done by supposing $\vec{w} = \left( x, y \right) \in W\implies
        \alpha\vec{w} = \left( \alpha x, \alpha y \right) \in W$.
\end{enumerate}
}

%%%%%%%%%%%%%%%%%%%%%% end another hide

\section{Operations with vector subspaces}
\begin{prop}
    If $W$, $Z\subsetV$ are vector subspaces of a vector space $V$, the sum of $W$ and $Z$, denoted by
    $W + Z$, is the smallest vector subspace of $V$ that contains both $W$ and $Z$.
\end{prop}

\begin{prop}
    If $W$, $Z\subseteq V$ are vector subspaces of a vector space $V$, the intersection of both subspaces,
    denoted by $W\cap Z$, is
    \begin{equation}
        \underbrace{W\cap Z}_{\textrm{This is also a vector subspace}} \bydef \{x\in \R\st x\in W\land x\in
        Z\}.
    \end{equation}
\end{prop}

\begin{proof}
    To prove that the intersection $W\cap Z$ is a vector subspace of $\Rtn$, we check the following subspace
    criteria:

    \begin{enumerate}
        \item The subspace $W\cap Z\neq \O$; i.e. the zero vector $\vec{0}$ of $\Rtn$ is in $W\cap Z$.

            As $W$ and $Z$ are subspaces of $\Rtn$, the zero vector $\vec{0}$ is in both $W$ and $Z$.
            Therefore, $\vec{0}\in W\cap Z\implies W\cap Z\neq\O$.

        \item For all $\vec{x}$, $\vec{y}\in W\cap Z\implies \vec{x} + \vec{y}\in W\cap Z$.

            Suppose $\vec{x}$, $\vec{y}\in W\cap Z$. Since $\vec{x}$, $\vec{y}\in W\cap Z\implies \vec{x}$,
            $\vec{y}\in W$ and $\vec{x}$, $\vec{y}\in Z$. Hence both $Z$ and $W$ are vector subspaces it
            follows that $\vec{x} + \vec{y}\in W$ and $\vec{x} + \vec{y}\in Z\implies \vec{x} + \vec{y}\in
            W\cap Z$.

        \item For all $\vec{x}\in W\cap Z, \alpha\in\R\implies \alpha\vec{x} W\cap Z$.

            Since $\vec{x}\in W\cap Z\implies \vec{x}\in W$ and $\vec{x}\in Z$, and since $W$ and $Z$ are
            vector subspaces, $\alpha\vec{x}\in W$ and $\alpha\vec{x}\in Z\implies \alpha\vec{x}\in W\cap Z$.
    \end{enumerate}
\end{proof}

\section{Basis and dimension of a vector space}
In vector spaces it's covenient considering subsets that generate all the space and that have the minimum
possible number of vectors. In some way, the \textit{size} of these sets determines the size of the vector
space.

\begin{defi}[Basis]
    A set $B$ of vectors is a basis of a vector space $V$ if $B$ is both linearly independent and a system of
    generators, $\gen{B} = V$.
\end{defi}

\begin{remark}
    For a given vector space there are multiple choices of basis, but all of them have the same cardinality.
\end{remark}

\begin{example}
    $B = \{\left( 1, 1 \right), \left( 1, -1 \right) \} $ is a basis of $\R^2$. To see if $B$ is system of
    generators one should study if
    \begin{equation}
        \left( x, y \right) = \lambda\left( 1, 1 \right) + \mu\left( 1, -1 \right)
    \end{equation}
    always has solutions $\lambda$, $\mu$, for any $\left( x, y \right) \in\R^2$. This lead us to

    \begin{equation}
        \sysdelim.\}\systeme{\lambda + \mu = x,\lambda - \mu = y} \quad\iff\quad \lambda = \frac{x + y}{2},\ \mu =
            \frac{x - y}{2}.
    \end{equation}

    Since there is always a solution, $B$ is a system of generators; i.e. $B$ generates $\R^2$. For checking
    if $B$ is linearly independent we consider
    \begin{equation}
        \left( 0, 0 \right) = \lambda\left( 1, 1 \right) + \mu\left( 1, -1 \right),
    \end{equation}
    that can only be solved with $\lambda = \mu = 0$.
\end{example}

\begin{example}
    Is $B = \{1, \sin x, x\}\subset \{\appl{f}{\R}{\R}\}$ a basis of $\gen{B}$? Note that, by definition,
    $B$ is a system of generators, therefore it's only necessary to prove that it is linearly independent.
    This is equivalent to prove that if $\lambda$, $\mu$, $\nu$ verifies
    \begin{equation}
        \lambda + \mu\sin x + \nu x = 0
    \end{equation}
    for all $x$, then $\lambda = \mu = \nu = 0$. Differentiating several times we obtain
    \begin{align}
        \lambda + \mu\sin x + \nu x &= 0, \\
        \mu\cos x + \mu &= 0, \\
        -\mu\sin x &= 0.
    \end{align}
    The last equation implies $\mu = 0$ and from the others we get $\lambda = \nu = 0$.
\end{example}

\begin{defi}[Dimension]
    A vector space has a \textbf{finite dimension} if it has a basis with a finite number of elements, and
    it corresponds to the cardinality of the basis.
    Otherwise, it has \textbf{infinite dimension}.
\end{defi}

\begin{remark}
    If $S$ is finite, then $V = \gen{S}\implies V$ has finite dimension.
\end{remark}

Some vector spaces of finite dimension are the following.

\begin{example}
    $B = \{\left( 1, 0, 0, \ldots, 0 \right) , \left( 0, 1, 0, \ldots, 0 \right) , \left( 0, 0, 1, \ldots, 0
    \right), \ldots, \left( 0, 0, 0, \ldots, 1 \right) \} $ is a basis of $\Rtn$, that is usually called
    canonical basis.
\end{example}

\begin{example}
    The space of all real functions, $\mathcal{F} = \{\appl{f}{\R}{\R}\}$ has infinite dimension.
\end{example}

Let's see now how to compute the basis of a vector space.
