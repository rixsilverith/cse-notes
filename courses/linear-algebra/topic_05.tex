\chapter{Vector spaces}

\begin{defi}[Vector space]
    A vector space over a field $F$ is a set $V$ together with two operations, $\appl{+}{V\times V}{V}$
    and $\appl{\cdot}{F\times V}{V}$, such that $\left( V, + \right) $ is an abelian group, and that for any
    $\vec{v_1}$, $\vec{v_2}\in V$, $\alpha_1$, $\alpha_2\in F$ the following properties are verified:
    \begin{align*}
        & 1\cdot \vec{v_1} = \vec{v_1}, \quad\quad\quad\quad & \left( \alpha_1 + \alpha_2 \right) \cdot \vec{v_1} = \alpha_1\vec{v_1} + \alpha_2\vec{v_1}, \\
        & \alpha_1\cdot\left( \alpha_2\cdot \vec{v_1} \right) = \left( \alpha_1\alpha_2 \right) \cdot
        \vec{v_1}, \quad\quad\quad\quad &  \alpha_1\cdot\left( \vec{v_1 + v_2} \right) = \alpha_1\vec{v_1} + \alpha_1\vec{v_2}.
    \end{align*}

    \hide{
    \begin{itemize}
        \item[] $1\cdot \vec{v_1} = \vec{v_1}$,
        \item[] $\alpha_1\cdot\left( \alpha_2\cdot \vec{v_1} \right) = \left( \alpha_1\alpha_2 \right) \cdot
            \vec{v_1}$,
        \hide{
        \item $\left( \alpha_1 + \alpha_2 \right) \cdot \vec{v_1} = \alpha_1\vec{v_1} + \alpha_2\vec{v_1}$,
        \item $\alpha_1\cdot\left( \vec{v_1 + v_2} \right) = \alpha_1\vec{v_1} + \alpha_1\vec{v_2}$.
        }
    }
    \end{itemize}
    \hide{
    A vector space over a field $F$ is a set $V$ together with two operations that satisfy the eight axioms
    listed below.
    \begin{itemize}
        \item The first operation, called \textbf{vector addition} $\appl{+}{V\times V}{V}$, takes any two
            vectors $\vec{v}$ and $\vec{w}$ and assigns to them a third vector written $\vec{v} + \vec{w}$,
            and called the sum of these two vectors. In this case, the resultant vector is also an element
            of the set $V$.
        \item The second operation, called \textbf{scalar multiplication} $\appl{\cdot}{F\times V}{V}$ takes
            any scalar $a$ and any vector $\vec{v}$ and gives another vector $a\vec{v}$. Similarly, the
            vector $a\vec{v}$ is an element of the set $V$.
    \end{itemize}
    }
\end{defi}

\hide{
\begin{remark}
    Scalar multiplication is not to be confused with the scalar product between two vectors.
\end{remark}
}

\begin{notation}
Elements in $F$ are usually called \textit{scalars} to differentiate them from the ones in $V$, that are
\textit{vectors}.
\end{notation}

\begin{example}
    $\Rtn$, with $n\in\Z^+$, is a vector space over $\R$ defined by
    \begin{equation}
        \Rtn\bydef \underbrace{\R\times\ldots\times\R}_{n \textrm{ times}} = \{\left( \alpha_1, \ldots,
    \alpha_n \right)\st \alpha_i\in\R\}
    \end{equation}
    with addition and scalar multiplication
    \begin{align}
        &\left( \alpha_1, \ldots, \alpha_n \right) + \left( \beta_1, \ldots, \beta_n \right) = \left( \alpha_1          + \beta_1, \ldots, \alpha_n + \beta_n\right) \\
        &\lambda\left( \alpha_1, \ldots, \alpha_n \right) = \left( \lambda\alpha_1, \ldots, \lambda\alpha_n
        \right).
    \end{align}
    This is the main vector space in which we will work in this topic.
\end{example}

From any field $F$, a new vector space can be defined over $F$ in the same manner as the previous example.

\begin{example}
    $m\times n$ matrices with coefficients in a field $F$, form the vector space
    \begin{equation}
        \mathcal{M}_{m\times n}\left( F \right) = \left\{
            \begin{bmatrix}
                a_{11} & a_{12} & \cdots & a_{1n} \\
                a_{21} & a_{22} & \cdots & a_{2n} \\
                \vdots & \vdots & \ddots & \vdots \\
                a_{m1} & a_{m2} & \cdots & a_{mn}
            \end{bmatrix} \st a_{ij}\in F,\ 1 \leq i \leq m,\ 1\leq j\leq n
        \right\}.
    \end{equation}
\end{example}
%In this topic we will work in $\R^n = \underbrace{\R\times\ldots\times\R}_{n \textrm{ times}}$, which a vector space over \R.

\begin{example}
    The solutions $\vec{x} = \left( x_1, \ldots, x_n \right) $ of the homogeneous system $A\vec{x} = B$
    with $A = \left( a_{ij} \right) \in F$ form a vector space over $F$.
\end{example}

\section{Vector subspaces}
\begin{defi}[Vector subspace] \label{def:vector-subspace}
    Let $V$ be a vector space over a field $F$. A nonempty subset $W\subset V$ defines a subspace of $V
    \iff \forall \lambda, \mu\in F$ and $\vec{w_1}, \vec{w_2}\in W\implies \lambda\vec{w_1} +
        \mu\vec{w_2}\in W$.
\end{defi}

To check if some subset $W$ of a vector space $V$ is a subspace of $V$ it's enough to see if $W$ accomplish
definition \ref{def:vector-subspace}. However, in many cases is easier to check that the following properties
are verified:
\begin{align} \label{prop:vector-subspace-2}
    \vec{w_1}, \vec{w_2}\in W &\quad\implies\quad \vec{w_1} + \vec{w_2}\in W, \\
    \vec{w}\in W,\ \lambda\in F&\quad\implies\quad \lambda\vec{w}\in W.
\end{align}

\begin{example}
    $W = \{\left( x, y, 0 \right)\in \R^3\st x, y\in\R\} $ is a vector subspace of $\R^3$, while $U = \{\left( x, y, 1 \right) \in\R^3\st x, y\in\R\} $ is not. Note that, for instance, $\left( 1, 1, 1 \right)\in U$ but
    $2\cdot\left( 1, 1, 1 \right) \not\in U$, which contradicts property \ref{prop:vector-subspace-2}.
\end{example}

\begin{example}
    $W = \{\left( x, y, z \right) \in\R^3\st x + y + z = 0, 2x - 3y - z = 0\} $ is a vector subspace of $\R^3$,
    but also of $V = \{\left( x, y, z \right) \in\R^3\st x + y + z = 0\} $.
\end{example}

In conclusion, whenever we set linear and homogeneous conditions in $\Rtn$, a subspace is obtained. This is
closely related to the description of a vector space as the solutions of an homogeneous system.

\begin{defi}[Linear combination]
    Let $\vec{v_1}$, $\ldots$, $\vec{v_n}\in V$ be vectors within a vector space $V$ over some field $F$. 
    A linear combination of $\vec{v_1}$, $\ldots$, $\vec{v_n}$ is an expression of the form $\alpha_1
    \vec{v_1} + \ldots + \alpha_n\vec{v_n}$, where  $\alpha_1$, $\ldots$, $\alpha_n\in F$.
\end{defi}

\begin{example}
    In $\R^2$ every vector $\vec{v} = \left(x, y\right)$ is a linear combination of $\vec{v_1} =
    \left( 1, 0 \right) $ and $\vec{v_2} = \left( 0, 1 \right) $ since $\vec{v} = x\vec{v_1} + y\vec{v_2}$.
\end{example}

\begin{prop}
    In a vector space, the identity element for vector addition, $\vec{0} = \left( 0, \ldots, 0 \right) $, is
    a linear combination of any vector $\vec{v}$, since $\vec{0} = 0\cdot\vec{v}$.
\end{prop}

\begin{defi}[Subspace generated by a set]
    Let $C$ be a subset of a vector space. The vector subspace generated by $C$, denoted by $\gen{C}$ or
    $\mathcal{L}\left( C \right) $, is the set of all the possible vector linear combinations of $C$.
\end{defi}

\begin{prop}
    If $C\subseteq V$, $\gen{C}$ is a vector subspace of $V$.
\end{prop}

\begin{example}
    In $\R^2$, $\gen{\left( 1, 0 \right) , \left( 0, 1 \right) } = \{\alpha_1\left( 1, 0 \right) +
    \alpha_2\left( 0, 1 \right) \} = \R^2 $, since we have already seen in a previous example that every
    vector of $\R^2$ is a linear combination of $\left( 1, 0 \right) $ and $\left( 0, 1 \right) $.
\end{example}

\begin{example}
    Let $C = \{\left( 1, 1, 2 \right), \left( 0, 2, -1 \right), \left( 1, 3, 1 \right) \subset\R^3\} $, then
    \begin{align}
        \gen{C} &= \{\lambda\left( 1, 1, 2 \right) + \mu\left( 0, 2, -1 \right) + \nu\left( 1, 3, 1 \right)\}
             \\ &= \{\left( \lambda + \nu, \lambda + 2\mu + 3\nu, 2\lambda - \mu + \nu \right) \st\lambda,
              \mu, \nu\in\R\},
    \end{align}
    since $\left( 1, 3, 1 \right) $ is a linear combination of $\left( 1, 1, 2 \right) $ and $\left( 0, 2, -1
    \right) $, more concisely
    \begin{equation}
        \left( 1, 3, 1 \right) = \left( 1, 1, 2 \right) + \left( 0, 2, -1 \right),
    \end{equation}
    the vector $\left( 1, 3, 1 \right) $ can be omitted; i.e., $\gen{C} = \gen{\left( 1, 1, 2 \right), \left(
    0, 2, -1\right) }$.
\end{example}

The previous example suggest defining the concept of some dependence between vectors in a set.

\begin{defi}[Linear independence] \label{defi:linear-independence}
    A set of vectors $\{\vec{v_1}, \ldots, \vec{v_n}\} $ from a vector space $V$ is \textbf{linearly
    independent} $\iff$ the equation $\alpha_1\vec{v_1} + \ldots + \alpha_n\vec{v_n} = \vec{0}$ can only
    be satisfied by $\alpha_i = 0$ for $i = 1, \ldots, n$. Otherwise, the set of vectors is
    \textbf{linearly dependent}.
\end{defi}

\begin{remark}
    Definition \ref{defi:linear-independence} implies that no vector in a set of linear independent vectors can be written as a linear combination of the remaining vectors in the set. In other words, even more concisely, a set of vectors is linear independent $\iff\vec{0}$ can be represented as a linear combination of its vectors in a unique way.
\end{remark}

\begin{note}
    Many times it's said that several vectors are linearly independent (or dependent), meaning that they
    form a linearly independent (or dependent) set, in this case we take as finite subset the one formed by
    themselves.
\end{note}

\begin{example}
    Vectors $\left( 1, 1, 0 \right)$, $\left( 2, 1, 1 \right)$, $\left( 5, 3, 2 \right)\in\R^3$ are not
    linearly independent. Writing the linear combination
    \begin{equation}
        \alpha_1\left( 1, 1, 0 \right) + \alpha_2\left( 2, 1, 1 \right) + \alpha_3\left( 5, 3, 2 \right) =
        \left( 0, 0, 0 \right)
    \end{equation}
    we reach the system
    \begin{equation}
    \sysdelim.\}\systeme{\alpha_1 + 2\alpha_2 + 5\alpha_3 = 0,\alpha_1 + \alpha_2 + 3\alpha_3 = 0,\alpha_2 + 2\alpha_3 = 0}
        \quad\implies\quad
            \alpha_2 &= -2\alpha_3,\
            \alpha_1 &= -\alpha_3
    \end{equation}
    and since solutions to this system depends on a parameter there are infinite solutions. Therefore, the
    vectors are linearly dependent.
\end{example}

\begin{prop}
    Any set of vectors containing the null vector, $\vec{0} = \left( 0, \ldots, 0 \right) $ is always
    linearly dependent.
\end{prop}

\begin{prop}
    In general, a non null vector $\vec{v}\in\R^n$ is linearly independent.
\end{prop}

\begin{proof}
The only solution to the equation $\alpha\vec{v} = \vec{0}$, $\alpha\in\R$ is $\alpha = 0$.
Suppose there is another solution with $\alpha\neq 0$. This implies $\exists \invers{\alpha}\in\R$, which
multiplied in both sides of the equation yields
\begin{equation}
    \alpha\vec{v} = \vec{0}\quad \iff\quad \invers{\alpha}\alpha\vec{v} = \invers{\alpha}\vec{0} \quad
        \iff\quad \vec{v} = \vec{0},
\end{equation}
and this is a contradiction. Then, if $\vec{v}\neq \vec{0}$, vector $\vec{v}$ is always linearly independent.
\end{proof}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% start another hide

\hide{
\begin{example}
To check if some vectors, let's say $\vec{v_1} = \left( 1, 2, -1 \right)$,
$\vec{v_2} = \left( 1, 0, 1\right) $ and $\vec{v_3} = \left( 2, 3, 2 \right)\in\R^3$, are linearly independent means finding non-trivial solutions to the equation
\begin{equation}
    \alpha_1\vec{v_1} + \alpha_2\vec{v_2} + \alpha_3\vec{v_3} = \vec{0},
\end{equation}
\begin{equation}
    \alpha_1\left( 1, 2, -1 \right) + \alpha_2\left( 1, 0, 1 \right) + \alpha_3\left( 2, 3, 2 \right) =
    \left( 0, 0, 0 \right).
\end{equation}
One can get these non-trivial solutions by solving the following homogeneous linear system of equations using
Gaussian elimination.
\begin{equation}
    \begin{cases}
        \alpha_1 + \alpha_2 + 2\alpha_3 = 0 \\
        2\alpha_2 + 0\alpha_2 + 3\alpha_3 = 0 \\
        -\alpha_1 + \alpha_2 + 2\alpha_3 = 0
    \end{cases}
    \quad\quad\quad
    A =
    \begin{bmatrix}
        1 & 1 & 2 \\
        -2 & 0 & 3  \\
        -1 & 1 & 2
    \end{bmatrix}
\end{equation}
\begin{equation}
    A^{*} =
    \begin{bmatrix}
        1 & 1 & 2 & 0 \\
        -2 & 0 & 3 & 0 \\
        -1 & 1 & 2 & 0
    \end{bmatrix}
    \sim
    \begin{bmatrix}
        1 & 1 & 2 & 0 \\
        0 & -2 & -3 & 0 \\
        0 & 0 & 0 & 0
    \end{bmatrix}
\end{equation}

\noindent Since $\rank{A} = \rank{A^*} = 2 < \textrm{ number of variables}$, by Rouché-Fröbenius theorem, there are
infinitely many solutions $\implies$ vectors $\vec{v_1}$, $\vec{v_2}$ and $\vec{v_3}$ are linearly dependent.
}
}

\hide{
\begin{prop}
    If a set of vectors $\vec{v_1}$, $\ldots$, $\vec{v_i}\in\R^n$ is linearly dependent then one of the
    vectors can be written as a linear combination of the others.
\end{prop}

\begin{proof}
Since $\vec{v_1}$, $\ldots$, $\vec{v_i}$ are linearly dependent there are scalars $\alpha_1$,
\ldots, $\alpha_i\in\R$ such that $\alpha_1\vec{v_1} + \ldots + \alpha_i\vec{v_i} = \vec{0}$ and at least one
of the $\alpha_i$-s is non zero. Suppose just to ease the notation that $\alpha_1\neq 0\implies \vec{v_1} =
\invers{\alpha_1}\left( -\alpha_2\vec{v_2} + \ldots + \left( -\alpha_i \right) \vec{v_i} \right) $.
\end{proof}
}

\hide{
\begin{coro}
    All vector spaces are equipped with at least two subspaces: the singleton set with the zero vector and
    the vector space itself. These are called \textbf{trivial subspaces} of the vector space.
\end{coro}

\noindent\textbf{Example.} Let $W = \{\left( x, y \right) \in\R^2 \st x + y = 0\}\subseteq \R^2 $. Is $W$ a
vector subspace of $\R^2$?
\begin{itemize}
    \item We first check if $W\neq \O$. $\left( 1, -1 \right) \in W\implies W\neq \O$.
    \item If $\vec{w_1} = \left( x, y \right) ^t$, $\vec{w_2} = \left( x', y' \right) ^t\in W\implies
        \vec{w_1} + \vec{w_2}\in W$? Suppose $\left( x + x' \right) + \left( y + y' \right) = \left( x + y \right) +
        \left( x' + y' \right) \underbrace{=}_{\vec{w_1}, \vec{w_2}\in W} 0 \implies \vec{w_1} + \vec{w_2}\in W$.
    \item If $\vec{w} = \left( x, y \right) ^t\in W$ and $\alpha\in\R\implies \alpha\vec{w}\in W$? Suppose $\alpha\vec{w} = \left(
        \alpha x, \alpha y\right) ^t \in W$. $\alpha x + \alpha y = \alpha\left( x + y \right) \underbrace{=}_{\vec{w}\in W} \alpha 0 = 0\implies \alpha\vec{w} \in W \implies W$ is a vector subspace of $\R^2$.
\end{itemize}
}

\hide{
\noindent\textbf{Example.} Is $W = \{\left( x, y \right) \in\R^2\st xy = 0\} $ a vector subspace of $\R^2$?
\begin{enumerate}
    \item Is $W\neq \O$? $\left( 1, 0 \right) \in W\implies W\neq \O$.
    \item If $\vec{w_1}$, $\vec{w_2}\in W\overset{?}{\implies} \vec{w_1} + \vec{w_2}\in W$.
        \begin{equation}
            \begin{cases}
                \textrm{let } \vec{w_1} = \left( 0, 1 \right)\in W \\
                \textrm{let } \vec{w_2} = \left( 1, 0 \right)\in W
            \end{cases}
            \quad\implies\quad \vec{w_1} + \vec{w_2} = \left( 1, 1 \right) \not\in W\quad\implies
        \end{equation}
        $\implies\quad W$ is not a vector subspace of $\R^2$.
    \item If 2 had been held, we would have checked if $\vec{w}\in W$ and $\alpha \in\R\overset{?}{\implies}
        \alpha\vec{w}\in W$. This can be done by supposing $\vec{w} = \left( x, y \right) \in W\implies
        \alpha\vec{w} = \left( \alpha x, \alpha y \right) \in W$.
\end{enumerate}
}

%%%%%%%%%%%%%%%%%%%%%% end another hide



\section{Basis and dimension of a vector space}
Taking about vector spaces it's covenient considering subsets that generate all the space and that have 
the minimum possible number of vectors as, in some way, the \textit{size} of these sets determines the size 
of the vector space.

\begin{defi}[Basis]
    A set $B$ of vectors is a basis of a vector space $V\iff B$ is both linearly independent and a system of
    generators, this is, the set $B$ generates the vector space; i.e. $\gen{B} = V$.
\end{defi}

\begin{remark}
    For a given vector space there are multiple choices of basis, but all of them have the same cardinality.
\end{remark}

\begin{example}
    \textbf{Prove that $B = \{\left( 1, 1 \right), \left( 1, -1 \right) \} $ is a basis of $\R^2$.}
    \begin{enumerate}
        \item To see if $B$ is system of generators one should study if
    \begin{equation}
        \left( x, y \right) = \lambda\left( 1, 1 \right) + \mu\left( 1, -1 \right)
    \end{equation}
    always has solutions $\lambda$, $\mu$, for any $\left( x, y \right) \in\R^2$. This lead us to

    \begin{equation}
        \sysdelim.\}\systeme{\lambda + \mu = x,\lambda - \mu = y} \quad\iff\quad \lambda = \frac{x + y}{2},\ \mu =
            \frac{x - y}{2}.
    \end{equation}

    Since there is always a solution, $B$ is a system of generators; i.e. $B$ generates $\R^2$. 

    \item For checking if $B$ is linearly independent we consider
    \begin{equation}
        \left( 0, 0 \right) = \lambda\left( 1, 1 \right) + \mu\left( 1, -1 \right),
    \end{equation}
    that can only be solved with $\lambda = \mu = 0$.
    \end{enumerate}
\end{example}

\begin{example}
    Is $B = \{1, \sin x, x\}\subset \{\appl{f}{\R}{\R}\}$ a basis of $\gen{B}$? Note that, by definition,
    $B$ is a system of generators, therefore it's only necessary to prove that it is linearly independent.
    This is equivalent to prove that if $\lambda$, $\mu$, $\nu$ verifies
    \begin{equation}
        \lambda + \mu\sin x + \nu x = 0
    \end{equation}
    for all $x$, then $\lambda = \mu = \nu = 0$. Differentiating several times we obtain
    \begin{align}
        \lambda + \mu\sin x + \nu x &= 0, \\
        \mu\cos x + \mu &= 0, \\
        -\mu\sin x &= 0.
    \end{align}
    The last equation implies $\mu = 0$ and from the others we get $\lambda = \nu = 0$.
\end{example}

\begin{defi}[Dimension]
    A vector space has a \textbf{finite dimension} if it has a basis with a finite number of elements, and
    it corresponds to the cardinality of any of its basis.
    Otherwise, it has \textbf{infinite dimension}.
\end{defi}

\begin{remark}
    If $S$ is finite, then $V = \gen{S}\implies V$ has finite dimension.
\end{remark}

\begin{example}
    $B = \{\left( 1, 0, 0, \ldots, 0 \right) , \left( 0, 1, 0, \ldots, 0 \right) , \left( 0, 0, 1, \ldots, 0
    \right), \ldots, \left( 0, 0, 0, \ldots, 1 \right) \} $ is a basis of $\Rtn$, usually called the
    canonical basis.
\end{example}

\begin{example}
    The space of all real functions, $\mathcal{F} = \{\appl{f}{\R}{\R}\}$ has infinite dimension.
\end{example}

Let's see now how to compute the basis of a vector space.

\begin{example}
    \textbf{Find a basis for $V = \{\left( x, y, z \right) \in\R^3\st x + y + z = 0\} $.}

    In this kind of problems it is better to express the conditions that define the subspace in terms of
    some variable that can take arbitrary values. In this case, note that
    \begin{equation}
        \vec{v}\in V\quad\iff\quad \vec{v} = \left( -y - z, y, z \right) 
    \end{equation}
    where $y$ and $z$ can take any real value. Therefore,
    \begin{align}
        \vec{v}\in V &\quad\implies\quad \vec{v} = y\left( -1, 1, 0 \right) + z\left( -1, 0, 1 \right) \\
                     &\quad\implies\quad \vec{v}\in\gen{\left( -1, 1, 0 \right), \left( -1, 0, 1 \right) }
    \end{align}
    which proves that $B = \{\left( -1, 1, 0 \right) , \left( -1, 0, 1 \right) \} $ is a system of generators.
    It is easy to see that $B$ is also linearly independent, then, $B$ is a basis of $V$.
\end{example}

\begin{theorem}[Steinitz's theorem] \label{thm:steinitz}
    Let $B = \{\vec{u_1}, \ldots, \vec{u_n}\} $ a basis of a vector space $V$, and let $\vec{v_1}, \ldots,
    \vec{v_m}$ be $m$ linearly independent vectors with $m \leq n$, then there exists $m$ vectors of $B$
    that can be replaced by $\vec{v_1}, \ldots, \vec{v_m}$, obtaining a new basis.
\end{theorem}

\begin{example}
    Consider the canonical basis of $\R^3$ $B = \{\left( 1, 0, 0 \right), \left( 0, 1, 0 \right), \left( 
    0, 0, 1\right) \} $ and the set of linearly independent vectors $S = \{\left( 0, 1, 0 \right), \left( 
    1, 2, 1\right) \} $. Following theorem \ref{thm:steinitz} we have $n = 3$, $m = 2$ and vectors
    \begin{equation}
        \vec{u_1} = \left( 1, 0, 0 \right), \quad\quad \vec{u_2} = \left( 0, 1, 0 \right),\quad\quad\vec{u_3}
        = \left( 0, 0, 1 \right)
    \end{equation}
    \begin{equation}
        \vec{v_1} = \left( 0, 1, 0 \right),\quad\quad \vec{v_2} = \left( 1, 2, 1 \right).
    \end{equation}
    Note that $\vec{u_2}$ and $\vec{u_3}$ can be replaced by $\vec{v_1}$ and $\vec{v_2}$ obtaining the basis
    of $\R^3$ given by $\hat{B} = \{\left( 1, 0, 0 \right), \left( 0, 1, 0 \right), \left( 1, 2, 1 \right)\}$,
    but if we replace $\vec{u_1}$ and $\vec{u_3}$ by $\vec{v_1}$ and $\vec{v_2}$ we don't get a new basis.
\end{example}

\begin{coro}
    In a vector space of finite dimension all basis have the same number of vectors.
\end{coro}

\begin{proof}
    If $B$ and $B'$ are basis with $m = \abs{B} < \abs{B'} = n$, then, following Steinitz's theorem 
    \ref{thm:steinitz} we could get a new basis $B''$ such that $B''\superset B$ with $B''\neq B$, but 
    since $\gen{B}$ is the whole space, this would imply that vectors in $B'' - B$ linearly dependent on 
    the ones in $B$ and therefore $B''$ is not linearly independent.
\end{proof}

\begin{defi}[Dimension]
    The dimension of a given vector space with finite dimension corresponds to the cardinality of any of
    its basis.
\end{defi}

\begin{note}
    Usually, this definition for dimension is completed by stating that the trivial vector space $V = \{\vec{0}\} $ has dimension zero.
\end{note}

The following corollary prevents us from checking if a possible basis is a system of generators if we know
beforehand the dimension of the vector space.

\begin{coro}
    In a vector space of dimension $n$, whatever $n$ linearly independent vectors form a basis.
\end{coro}

\begin{example}
    \textbf{Prove that $B = \{\left( 1, 2, 1 \right) , \left( 1, 2, 0 \right), \left( 0, 1, 1 \right)\} $ is
    a basis of $\R^3$.}

    Using the previous corollary is enough to see that the vectors in $B$ are linearly independent, since 
    $\dim \R^3 = 3$.
\end{example}

Knowing that any vector can be written as a linear combination of the elements of a basis, it is worth 
defining the numbers that appear as coefficients, which characterises the vector.

\begin{defi}[Coordinates]
    Let $V$ be a vector space of finite dimension and $B = \{\vec{u_1}, \ldots, \vec{u_n}\} $ one of its
    basis. Then, the coordinates of certain vector $\vec{v}\in V$ in basis $B$ are $\left( \lambda_1, \ldots,
    \lambda_n\right) $, if $\vec{v} = \lambda_1\vec{u_1} + \ldots + \lambda_n\vec{u_n}$.
\end{defi}

\begin{note}
    Many times it is written $\vec{v} = \left( \lambda_1, \ldots, \lambda_n \right) $. When using this
    notation it must be clear which is the basis we are considering. Also, it is worth noting that coordinates
    depend on the order of the elements which form the basis. Therefore, more consisely, coordinates are not
    only associated to the basis, but also to the way its elements are ordered.
\end{note}

\section{Operations with vector subspaces. Grassman's formula}
\begin{prop}
    If $W$, $Z\subsetV$ are two vector subspaces of a vector space $V$, the sum of $W$ and $Z$, denoted by
    $W + Z$, is the smallest vector subspace of $V$ that contains both $W$ and $Z$. In other words,
    \begin{equation}
        W + Z \bydef \{\vec{x}\st \vec{x} = \vec{w} + \vec{z},\ \vec{w}\in W,\ \vec{z}\in Z\}. 
    \end{equation}
\end{prop}

\begin{prop}
    If $W$, $Z\subseteq V$ are vector subspaces of a vector space $V$, the intersection of both subspaces,
    denoted by $W\cap Z$, is
    \begin{equation}
        W\cap Z \bydef \{x\in \R\st x\in W\land x\in Z\}.
    \end{equation}
\end{prop}

\begin{remark}
    Note that the two subspace are never disjoint since the zero vector $\vec{0}$ is within every subspace.
\end{remark}

\begin{lemma}
    If $W$ and $Z$ are subspaces of a vector space $V$, then $W + Z$ and $W\cap Z$ are also vector spaces.
\end{lemma}

\begin{proof}
    To prove that the intersection $W\cap Z$ is a vector subspace of $\Rtn$, we check the following subspace
    criteria:

    \begin{enumerate}
        \item The subspace $W\cap Z\neq \O$; i.e. the zero vector $\vec{0}$ of $\Rtn$ is in $W\cap Z$.

            As $W$ and $Z$ are subspaces of $\Rtn$, the zero vector $\vec{0}$ is in both $W$ and $Z$.
            Therefore, $\vec{0}\in W\cap Z\implies W\cap Z\neq\O$.

        \item For all $\vec{x}$, $\vec{y}\in W\cap Z\implies \vec{x} + \vec{y}\in W\cap Z$.

            Suppose $\vec{x}$, $\vec{y}\in W\cap Z$. Since $\vec{x}$, $\vec{y}\in W\cap Z\implies \vec{x}$,
            $\vec{y}\in W$ and $\vec{x}$, $\vec{y}\in Z$. Hence both $Z$ and $W$ are vector subspaces it
            follows that $\vec{x} + \vec{y}\in W$ and $\vec{x} + \vec{y}\in Z\implies \vec{x} + \vec{y}\in
            W\cap Z$.

        \item For all $\vec{x}\in W\cap Z, \alpha\in\R\implies \alpha\vec{x} W\cap Z$.

            Since $\vec{x}\in W\cap Z\implies \vec{x}\in W$ and $\vec{x}\in Z$, and since $W$ and $Z$ are
            vector subspaces, $\alpha\vec{x}\in W$ and $\alpha\vec{x}\in Z\implies \alpha\vec{x}\in W\cap Z$.
    \end{enumerate}
\end{proof}

\begin{prop}[Grassman's formula]
    If $W$ and $Z$ are two subspaces of a vector space with finite dimension, then
    \begin{equation}
        \dim\left( W + Z \right) = \dim W + \dim Z - \dim\left( W\cap Z \right). 
    \end{equation}
\end{prop}
