\bchapter{Linear transformations}

One of the main goals of linear algebra is the characterization of the solutions to the set of $m$
linear equations in $n$ unknowns $x_1$, $\ldots$, $x_n$. Linear transformations, also called linear maps,
and their properties give us a lot of insight into the characteristics of the solutions to a system of
linear equations.

\section{Linear maps between vector spaces}

%Let $V$ and $W$ be vector spaces over a field $F$.

\begin{defn}[Linear transformation]
    Let $V$ and $W$ be two vector spaces over the same field $F$. A function $\appl{T}{V}{W}$ is a
    \textit{linear transformation}, or \textit{linear map}, if it preserves addition and scalar multiplication
    between the two vector spaces; in other words, if
    \begin{align}\label{def:linear-trans:eq1}
        T\left( \vec{v} + \vec{u} \right) &= T\left( \vec{v} \right) + T\left( \vec{u} \right) \quad
                                          &\forall\ \vec{v}, \vec{u}\in V, \\
        T\left( \lambda\vec{v} \right) &= \lambda T\left( \vec{v} \right) \quad &\forall \lambda\in
        F,\ \vec{v}\in V. \label{def:linear-trans:eq2}
    \end{align}
\end{defn}

\noindent Equations (\ref{def:linear-trans:eq1}) and (\ref{def:linear-trans:eq2}) can be summarized into
\begin{equation}
    T\left( \lambda\vec{v} + \mu\vec{u} \right) = \lambda T\left( \vec{v} \right) + \mu T\left( \vec{u}\right)
    \quad\quad \forall\ \lambda, \mu\in F,\ \vec{v}, \vec{u}\in V,
\end{equation}
which, for any vectors $\vec{v_1}$, $\ldots$, $\vec{v_n}\in V$ and scalars $\lambda_1$, $\ldots$, $\lambda_n$, can be written in a more general way as
\begin{equation}
    T\left( \sum_{i=1}^n \lambda_i\vec{v_i}\right) = \sum_{i=1}^n \lambda_i T\left( \vec{v_i} \right).
\end{equation}

\noindent Thus, a linear transformation is said to be \textit{operation preserving}. That is, it doesn't matter
whether the linear transformation is applied before or after the operations of addition and scalar
multiplication, and therefore, linear combinations are preserved between the two vector spaces.

\hide{
Linear transformations are useful because they preserve the structure of a vector space. So, many qualitative
assessments of a vector space that is the domain of a linear transformation may, under certain conditions,
automatically hold in the image of the linear transformation.
}

\hide{
If $T$ and $R$ are two linear transformations from $V$ to $W$, and $\lambda$, $\mu$ two scalars, the
following properties hold:
\begin{itemize}[itemsep = -2pt]
    \item $T\left( \lambda\vec{v} + \mu\vec{u} \right) = \lambda T\left( \vec{v} \right) + \mu T\left( \vec{u}
        \right) $; which can be generalized as
        \begin{equation}
            T\left( \lambda_1\vec{v_1} + \ldots + \lambda_n\vec{v_n} \right) = T\left( \sum_{i=1}^n \lambda_i
                \vec{v_i}\right) = \sum_{i=1}^n \lambda_i T\left( \vec{v_i} \right) = \lambda_1 T\left(
            \vec{v_1} \right) + \ldots + \lambda_n T\left( \vec{v_n} \right).
        \end{equation}.
    \item $T\left( \vec{0} \right) = \vec{0}$ (note that $\vec{0} = 0\vec{v}$ for any $\vec{v}$).
    \item $T\left( -\vec{v} \right) = -T\left( \vec{v} \right) $.
    \item The transformation $\appl{\lambda T}{V}{W}$, defined by $\left( \lambda T \right)\left( \vec{v}
        \right) = \lambdaT\left( \vec{v} \right)$, is linear.
    \item The transformation $\appl{T + R}{V}{W}$, defined by $\left( T + R \right)\left( \vec{v} \right) =
        T\left( \vec{v} \right) + R\left( \vec{v} \right) $, is linear.
    \item If $\appl{T}{V}{W}$ and $\appl{R}{W}{Z}$ are linear, then $\appl{R\circ T}{V}{Z}$ is linear.
\end{itemize}
}

\begin{notation}
    A linear transformation is sometimes referred to as a \textit{vector space homomorphism}.
\end{notation}

\begin{defn}
    A linear transformation $\stdlt$ is
    \begin{itemize}[itemsep = -2pt]
        \item a \textit{monomorphism} if it is injective; i.e. $T\left( \vec{v} \right) = T\left(
            \vec{u}\right) \iff \vec{v} = \vec{u}$.
        \item an \textit{epimorphism} if it is surjective; i.e. $\forall\ \vec{w}\in W $, $\exists\
            \vec{v}\in V\st T\left( \vec{v} \right) = \vec{u}$.
        \item an \textit{isomorphism} if it is bijective; i.e. $\forall\ \vec{w}\in W$, $\uexists\vec{v}\in V
            \st T\left( \vec{v} \right) = \vec{u}$.
    \end{itemize}
\end{defn}

\begin{example}
    For all vector space $V$ the indentity transformation
    \begin{equation}
        \appl{\mop{Id}_{V}}{V}{V},\quad\quad \mop{Id}_V\left( \vec{v} \right) = \vec{v},\quad \forall\vec{v}
        \in V
    \end{equation}
    is linear. More generally, if $V$ is a vector space over a field $F$ and we take a fixed scalar
    $\lambda\in F$, the transformation
    \begin{align}
        \appl{T_\lambda}{V}{V}\quad\quad \vec{v}\longmapsto T_\lambda\left( \vec{v} \right) = \lambda\vec{v}
    \end{align}
    is linear. In particular, the transformation that maps any vector of $V$ to the vector $\vec{0}\in W$ is
    linear, and is called \textit{zero map}. We call \textit{homothety} in $V$ any linear transformation from
    $V$ to $V$ of the form $T_\lambda\left( \vec{v} \right) = \lambda\vec{v}$, where $\lambda$ is the
    \textit{ratio} of the homothety.
\end{example}

\begin{example}
    If $A$ is a real $m\times n$ matrix, then $A$ defines a linear transformation from $\R^n$ to $\R^m$ by
    sending the column vector $\vec{x}\in\R^n$ to the column vector $A\vec{x}\in\R^m$.
\end{example}

\begin{example}
    Differentiation defines a linear map from the space of all differentiable functions to the space of all
    functions.
    \begin{equation}
        \dv{x}\left( \sum_{i=1}^n c_i f_i\left( x \right)  \right) = \sum_{i=1}^n c_i\dv{f_i\left( x
        \right) }{x}.
    \end{equation}
\end{example}

\section{Representation in terms of matrices}
Linear transformations are mostly commonly written in terms of matrix multiplication. If $V$ and $W$ are two
finite-dimensional vector spaces and a basis is defined for each of them, then every linear transformation
from $V$ to $W$ can be represented by a matrix. This fact is useful to perform concrete computations.

\hide{
\begin{defi}
    The matrix representing the linear transformation $\appl{T}{V}{W}$ relative to the bases $B_V = \{
    \vec{v_1}, \ldots, \vec{v_n}\} $ for $V$ and $B_W = \{\vec{w_1}, \ldots, \vec{w_m}\} $ for $W$ is a
    matrix $A\in\mathcal{M}$
\end{defi}

Let $B_V = \{\vec{v_1}, \ldots, \vec{v_n}\} $ and $B_W = \{\vec{w_1}, \ldots, \vec{w_m}\} $ be a basis for
$V$ and $W$ respectively. Then, every vector $\vec{v}\in V$ is uniquely determined by a linear combination
of vectors in $B_V$,
\begin{equation}
    \left[\vec{v}\right]_{B_V} = \hvect{\lambda_1, \ldots, \lambda_n} = \sum_{i=1}^n\lambda_i\vec{v_i}.
\end{equation}
Likewise, $\forall\vec{w}\in W$,
\begin{equation}
    \left[\vec{w}\right]_{B_W} = \hvect{\mu_1, \ldots, \mu_n} = \sum_{i=1}^m\mu_i\vec{w_i}.
\end{equation}


Let $\appl{T}{V}{W}$. If
\begin{equation}
T\left( \vec{v_i} \right) = \sum_{j=1}^n a_{ji}\vec{w_i}
\end{equation}
}


\begin{prop}
    Fixed basis $B_V$ and $B_W$ for two finite-dimension vector spaces $V$ and $W$, respectively, over a field
    $F$, any linear transformation $\appl{T}{V}{W}$ can be written as $T\left( \vec{v} \right) = A\vec{v}$, where $A\in\mathcal{M}_{m\times n}\left( F\right)$
    with $m = \dim W$ and $n = \dim V$. Columns of $A$ are vectors $T\left( \vec{v_i} \right)$ where
    vectors $\vec{v_i}\in B_V$.
\end{prop}

\begin{proof}
    Let $\{\vec{v_1}, \ldots, \vec{v_n}\} $ be a basis for $V$. Then every vector $\vec{v}\in V$ is uniquely
    determined by the coefficients $\lambda_1, \ldots, \lambda_n$ in the field $F$. If $\appl{T}{V}{W}$ is
    a linear transformation,
    \begin{equation}
        T\left( \sum_{i=1}^n\lambda_i \vec{v_i} \right) = \sum_{i=1}^n\lambda_i T\left( \vec{v_i} \right),
    \end{equation}
    which implies that the transformation $T$ is entirely determined by vectors $T\left( \vec{v_1} \right)$,
    $\ldots$, $T\left( \vec{v_i} \right) $. Now let $\{\vec{w_1}, \ldots, \vec{w_m}\} $ be a basis for $W$.
    Then we can represent each vector $T\left( \vec{v_j} \right) $ as
    \begin{equation}
        T\left( \vec{v_j} \right) = \sum_{i=1}^m a_{ij}\vec{w_i}.
    \end{equation}
    Thus, the transformation $T$ is entirely determined by the values of $a_{ij}$. If we put these values
    into an $m\times n$ matrix $A$, then we can conveniently use it to compute the vector output of $T$ for
    any vector in $V$. To get $A$, every column $j$ of $A$ is a vector $T\left( \vec{v_j} \right) $
\end{proof}

%A transformation $\appl{T}{V}{W}$ from a $m$-dimensional vector space $V$ to a $n$-dimensional vector space $W$ is given
%by an $n\times m$ matrix $A$. However, this requires choosing a basis for $V$ and a basis for $W$, while
%the linear transformation exists independent of basis.
In other words, fixed basis
    $B_V$ and $B_W$ of $V$ and $W$ respectively, coordinates of a vector $\vec{v}$ in basis $B_V$ and the ones
    of $T\left( \vec{v} \right) $ in $B_W$ are related by the multiplication of a certain matrix, whose columns
    are the coordinates in $B_W$ of the images of the elements in $B_V$.

\hide{
\begin{prop}
    Any linear transformation $\appl{T}{V}{W}$ between two vector spaces $V$ and $W$ over a field $F$ is of
    the form $T\left( \vec{v} \right) = A\vec{v}$ with $A\in\mathcal{M\left( F \right) }$. Columns of $A$ are
    vectors $T\left( \vec{e_i} \right) $ with $\{\vec{e_1}, \ldots, \vec{e_n}\} $ being the canonical basis
    of $V$.
\end{prop}

\begin{proof}
    Suppose $V = \R^m$, $W = \R^n$ are two vector spaces. If $\{\vec{e_1}, \ldots, \vec{e_n}\} $ is a
    standard basis of $V$, every $\vec{v}\in V$ is of the form
    \begin{equation}
        \vec{v} = \left( v_1, \ldots, v_n \right) ^T = \sum_{j = 1}^n v_j\vec{e_j}\quad\implies\quad
        T\left( \vec{v} \right) = \sum_{j = 1}^n v_j T\left( \vec{e_j} \right).
    \end{equation}
    Given $T$, vectors $T\left( \vec{e_j} \right) \in W$ are constants $T\left( \vec{e_j} \right) = \left(
    a_{1j}, a_{2j}, \ldots, a_{mj}\right) ^T$. Therefore, the $i$-th coordinate of $T\left( \vec{v} \right)$
    is given by
    \begin{equation}
        \sum_{j=1}^n a_{ij}v_j.
    \end{equation}
\end{proof}
}

\begin{example}
    The linear transformation from $\R^3$ to $\R^2$ defined by $T\left( x, y, z \right) = \left( x - y,
    y - z\right) $ is given by the matrix
    \begin{equation}
        M = \begin{bmatrix}
            1 & -1 & 0 \\
            0 & 1 & -1
        \end{bmatrix} .
    \end{equation}
    So $T$ can also be defined for vectors $\vec{v} = \left( x, y, z \right)^T $ by the matrix product
    \begin{equation}
        T\left( \vec{v} \right) = \begin{bmatrix}
            1 & -1 & 0 \\
            0 & 1 & -1
        \end{bmatrix}\begin{bmatrix}
            x \\ y \\ z
        \end{bmatrix}.
    \end{equation}
    Here, the dimension of the initial vector space corresponds to the number of columns in the matrix, while
    the dimension of the target vector space is the number of rows in the matrix.
\end{example}



\section{Kernel and image. Rank-nullity theorem}
Related to a linear transformation $\appl{T}{V}{W}$ from a vector space $V$ to a vector space $W$, two
subspaces of $V$ and $W$, the \textit{kernel} and the \textit{image} of $T$ respectively, are defined.

\begin{defn}
    Let $\appl{T}{V}{W}$ be a linear transformation, then the \textit{kernel} and the \textit{image} of $T$
    are defined as
    \begin{align}
        \kernel \left( T\right) &\bydef \{\vec{v}\in V\st T\left( \vec{v} \right) = \vec{0}\}, \\
        \image \left( T\right) &\bydef \{\vec{y}\in W\st T\left( \vec{x} \right) = \vec{y},\ \vec{x}\in V\}.
    \end{align}
\end{defn}

\begin{notation}
    The kernel of a linear transformation is also known as \textit{null space}, denoted by $\mop{Null}
    \left( T \right) = \kernel\left( T \right) $.
\end{notation}

\begin{prop}
    Let $\appl{T}{V}{W}$ be a linear transformation. Then the image of $T$ is a subspace of $W$ and the
    kernel of $T$ is a subspace of $V$.
\end{prop}

\begin{proof}
    We have to show that both the kernel and the image are closed under addition and scalar multiplication.

    \noindent For the kernel, if $\vec{v}$, $\vec{u}\in\kernel T$; i.e. $T\left( \vec{v} \right) = T\left( \vec{u}
    \right) = \vec{0}$, since $T$ is linear,
    \begin{align}
        T\left( \vec{v} + \vec{u} \right) &= T\left( \vec{v} \right) + T\left( \vec{u} \right) = \vec{0} +
        \vec{0} = \vec{0}\in\kernel T, \\
        T\left( \lambda\vec{v} \right) &= \lambda T\left( \vec{v} \right) = \lambda\vec{0} = \vec{0}\in\kernel
        T, \quad \forall\lambda\in F.
    \end{align}
    Therefore, $\kernel T\subset V$ is a vector subspace.

    \noindent For the image, let $\vec{w_1}$, $\vec{w_2}\in\image T$ and $\lambda\in F$. By definition, there exist
    $\vec{u_1}$, $\vec{u_2}\in V$ such that $\vec{w_1} = T\left( u_1 \right) $ and $\vec{w_2} = T\left(
    \vec{u_2} \right) $. Since $T$ is linear,
    \begin{align}
        T\left( \vec{u_1} + \vec{u_2} \right) &= T\left( \vec{u_1} \right) + T\left( \vec{u_2} \right) =
        \vec{w_1} + \vec{w_2}, \\
        T\left( \lambda\vec{u_1} \right) &= \lambda T\left( \vec{u_1} \right) = \lambda\vec{w_1}.
    \end{align}
    Therefore, $\image T$ is a vector subspace of $W$.
\end{proof}

\begin{prop}
    Let $\stdlt$ be a linear transformation. Then, the following properties hold.
    \begin{itemize}[itemsep = -2pt]
        \item $T$ is a monomorphism $\iff\kernel T = \{\vec{0}\}$.
        \item $T$ is an epimorphism $\iff\image T = W$.
        \item $T$ is an isomorphism $\iff\image T = W$ and $\kernel T = \{\vec{0}\}$.
    \end{itemize}
\end{prop}

\begin{proof}
    To prove the first point is enough to note that $T\left( \vec{v_1} \right) = T\left( \vec{v_2} \right)$
    with different $\vec{v_1}$, $\vec{v_2}\in V$ implies $T\left( \vec{v} \right) = \vec{0}$ with $\vec{v} =
    \vec{v_1} - \vec{v_2}\neq \vec{0}$. The other way around, if $\vec{v} \neq\vec{0}\in\kernel T$ we could
    take $\vec{v_1} = \vec{v}$, $\vec{v_2} = \vec{0}$.

    The last two points are obvious.
\end{proof}

\begin{defn}
    Let the \textit{rank} of $T$ be $\rank{T} = \dim \left( \image T \right)$ and the
    \textit{nullity} of $T$ be $\nullity{T} = \dim\left( \kernel T \right) $.
\end{defn}

\begin{theorem}[Rank-Nullity Theorem]
    Let $\appl{T}{V}{W}$ be a linear transformation. Then $\rank{T} + \nullity{T} = \dim V$.
\end{theorem}

\begin{prop}
    If $V$ and $W$ are finite-dimensional vector spaces, $\appl{T}{V}{W}$ is a linear transformation,
    $\mathcal{B}_V = \{\vec{v_1}, \ldots, \vec{v_n}\}\subset V $ a basis of $V$, $\mathcal{B}_W = \{\vec{w_1},
    \ldots, \vec{w_m}\}\subset W $ a basis of $W$ and $A\in\mathcal{M}_{m\times n}\left( F \right)$ the matrix
    of the transformation $T$ in bases $\mathcal{B}_V$ and $\mathcal{B}_W$, then $\dim\left( \image T \right)
    = \rank{A}$.
\end{prop}

\hide{
In order to compute the dimension of these vector subspaces we need first to know the range of the matrix
of $T$ independent of basis.

\begin{prop}
    Let $A$ be the matrix of a linear transformation \stdlt for a finite number of basis of $V$ and $W$,
    then
    \begin{equation}
        \dim \kernel T = \dim V - \rg A \quad\quad\quad \dim\image T = \rg A.
    \end{equation}
\end{prop}
}

\begin{coro}
    Suppose $\dim V = \dim W = n < \infty$. A linear transformation $\stdlt$ is bijective $\iff$ its
    matrix $A$ verifies $\rank{A} = n$.
\end{coro}

\hide{
\begin{theorem}
    If $\appl{T}{V}{W}$ is a linear transformation from a vector space $V$ to a vector space $W$, then
    $\ker T$ is a vector subspace of $V$.
\end{theorem}

\begin{proof}
    \begin{itemize}
        \item Since $T$ is a linear transformation, $T\left( \vec{0} \right) = \vec{0}\implies \vec{0}\in
            \ker T\implies \ker T\neq \O$.
        \item If $\vec{v}, \vec{u}\in \ker T\implies \vec{v} + \vec{u}\in \ker T$? $T\left( \vec{v} + \vec{u}
            \right) = T\left( \vec{v} \right) + T\left( \vec{u} \right) =
            \vec{0} + \vec{0} = \vec{0}\in \ker T$.
        \item If $\vec{v}\in\ker T$ and if $\lambda\in F\implies \lambda\vec{v}\in\ker T$? $T\left( \lambda
            \vec{v}\right) = \lambda T\left( \vec{v} \right) = \lambda\vec{0} = \vec{0}$.
    \end{itemize}
    Since $\ker T$ holds the three properties, $\ker T$ is a vector subspace of $V$.
\end{proof}
}

\section{Change of basis}
