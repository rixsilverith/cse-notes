\bchapter{Endomorphisms}

\section{Eigenvectors and eigenvalues}
Let $V$ be a finite-$n$-dimensional vector space over a field \field, we denote the vector space of the
endomorphisms of $V$ by $\mop{End}\left( V \right)$.

\begin{defn}[Eigenvalue / Eigenvector]
    Given an endomorphism, $T\in\mop{End}\left( V \right)$, a non-zero vector $\vec{v}\in V$ is said to be an
    \textbf{eigenvector} of $T$ if there exists $\lambda\in\field$ such that $T\left( \vec{v} \right)= \lambda
    \vec{v}$. If $\vec{v}\in V$ is an eigenvector of the endomorphism $T$, then $\lambda$ is called an
    \textbf{eigenvalue} of $T$.
\end{defn}

\hide{
\begin{example}
    If $T\in\endospace{V}$ and $\vec{v}\in\kernel{T}$ with $\vec{v} \neq\vec{0}$ then $\vec{v}$ is an
    eigenvector of $T$ with eigenvalue $0$. In particular, if $T$ is an automorphism, there is no
    eigenvector with eigenvalue equal to $0$.
\end{example}
}

    If $T\in\endospace{V}$ and $\vec{v}\in V$ is an eigenvector of $T$ with eigenvalue $\lambda$, then
    $\forall t\in\field$, $t\neq 0$, the vector $\vec{w} = t\vec{v}$ is also an eigenvalue of $T$ with the
    same eigenvalue $\lambda$.
    \begin{equation}
        T\left( \vec{w} \right) = T\left( t\vec{v} \right) = tT\left( \vec{v} \right) = t\left(
        \lambda\vec{v}\right) = \lambda\left( t\vec{v} \right) = \lambda\vec{w}.
    \end{equation}
    In other words, if $\vec{v}\in V$ is an eigenvector of the endomorphism $T$ with eigenvalue $\lambda$,
    every non-zero vector in the linear subspace $\gen{\vec{v}}$ is an eigenvector of $T$ with the same
    eigenvalue. More generally, if $T\in\endospace{V}$ and $\vec{v_1}$, $\ldots$, $\vec{v_m}\in V$ are
    eigenvectors of $T$ with the same eigenvalue $\lambda\in F$, then every non-zero vector in the
    linear subspace $\gen{\vec{v_1}, \ldots, \vec{v_m}}$ is an eigenvalue of $T$ with eigenvalue $\lambda$.

\hide{
\begin{proposition}
    If $T\in\endos{V}$, $\vec{v_1}$, $\vec{v_2}\in V$ are eigenvectors with eigenvalues $\lambda_1$,
    $\lambda_2\in F$ and $\lambda_1\neq\lambda_2$ then $\vec{v_1}$ and $\vec{v_2}$ are linearly
    independent.
\end{proposition}

\begin{proof}
    If they were linearly dependent, we would have that $\vec{v_2}\in\gen{\vec{v_1}}$ and $T\left(
    \vec{v_2} \right) = \lambda_1\vec{v_2}$, but $T\left( \vec{v_2} \right) = \lambda_2\vec{v_2}$,
    from which $\left( \lambda_1 - \lambda_2 \right)\vec{v_2} = \vec{0}$ for $\vec{v_2}\neq\vec{0}$
    (since it is an eigenvector), contradicting $\lambda_1\neq\lambda_2$.
\end{proof}
}

\begin{prop}
    Let $T\in\endospace{V}$ and $\vec{v_1}$, $\ldots$, $\vec{v_m}\in V$ eigenvectors of $T$ with eigenvalues
    $\lambda_1$, $\ldots$, $\lambda_m$, each of them different, then $\vec{v_1}$, $\ldots$, $\vec{v_m}$
    are linearly independent.
\end{prop}

\begin{proof}
    Let's prove by induction. If $m = 1$, since $\vec{v_1}\neq\vec{0}$, there's nothing to prove. Let
    $a_1\vec{v_1} + \ldots + a_m\vec{v_m} = \vec{0}$, then
    \begin{align}
        \vec{0} &= \left( T - \lambda_1 I \right)\left( a_1\vec{v_1} + \ldots + a_m\vec{v_m} \right) \\ &=
        a_2\left( \lambda_2 - \lambda_1 \right)\vec{v_2} + \ldots + a_m\left( \lambda_m - \lambda_1
        \right)\vec{v_m}.
    \end{align}
    By the induction hypothesis $\vec{v_2}, \ldots, \vec{v_m}$ are linearly independent, then $a_j\left(
    \lambda_j - \lambda_1\right) = 0$ for each $j = 2, \ldots, m$. But $\lambda_j\neq\lambda_1$ if
    $j\neq 1$, therefore $a_j = 0$ for $j = 2, \ldots, m$. Finally, lasts $a_1\vec{v_1} = \vec{0}$ with
    $\vec{v_1}\neq \vec{0}$, from where $a_1 = 0$ also.
\end{proof}

\begin{coro}
    If $V$ is an $n$-dimensional vector space and $T\in\endospace{V}$, the number of different eigenvalues
    of $T$ is $\leq n$. If there are exactly $n$ different eigenvalues, $\lambda_1$, $\ldots$, $\lambda_n$,
    the set of the corresponding eigenvectors $\{\vec{v_1}, \ldots, \vec{v_n}\}$ is a basis of $V$. In
    this basis the matrix of the endomorphism $T$ is a diagonal one of the form
    \begin{equation}
        \begin{bmatrix} \lambda_1 & 0 & 0 \\ 0 & \ddots & 0 \\ 0 & 0 & \lambda_n \end{bmatrix}.
    \end{equation}
\end{coro}

\begin{defn}[Diagonalizable endomorphism]
    An endomorphism $T\in\endospace{V}$ is said to be \textbf{diagonalizable} over \field if there exists an
    ordered basis of $V$ consisting on eigenvectors of $T$.
\end{defn}

\noindent In other words, an endomorphism is diagonalizable if there exists a basis of $V$ in which the matrix
of the endomorphism is diagonal. In the same way, a matrix $A\in\mathcal{M}_n\left( \field \right)$ is
diagonalizable over \field if the endomorphism $T\left( \vec{v} \right) = A\vec{v}$ is diagonalizable over
\field. From this equivalent definition, $A\in\mathcal{M}_n\left( \field \right)$ is diagonalizable over
\field if there exists a matrix $P\in\mathcal{M}_n\left( \field \right) $ with non-zero determinant such
that $D = \invers{P}AP$ is a diagonal matrix.

\hide{
Another equivalent definition would be that an endomorphism $T\left( \vec{v}
\right) = A\vec{x}$ is diagonalizable if its corresponding matrix can be written as $A = PD\invers{P}$, where
$P$ is an invertible matrix whose columns are the coordinates of the basis vectors of $A$, and $D$ is a
diagonal matrix.
}

Diagonalizable endomorphisms are especially easy for computations, once their eigenvalues and eigenvectors
are known.

\section{Eigenvalue and eigenvector computation. Characteristic polynomial and eigenspaces}
Let $T$ be an endomorphism. If we know an eigenvalue $\lambda$, computing the corresponding eigenvectors is
equivalent to solving $T\left( \vec{v} \right) = \lambda\vec{v}$, which is nothing more than solving a system
of linear equations. To be more precise, if $T\left( \vec{v} \right) = A\vec{v}$ and $\lambda$ is an
eigenvalue, the corresponding eigenvectors are the non-trivial solutions to $A\vec{v} = \lambda\vec{v}$, which
can be rewritten as $\left( A - \lambda I \right)\vec{v} = \vec{0}$. This is, the corresponding eigenvectors
to a certain eigenvalue $\lambda$ are the non-zero vectors of $\kernel\left( T - \lambda I \right) $, where
$I$ is the identity matrix. This kernel is known as the \textbf{eigenspace} of $\lambda$, and its denoted by
$E\left( \lambda \right) $. Since $\vec{v}$ must be non-zero, the matrix $A - \lambda I$ has a non-zero kernel.
Thus, the matrix is not invertible, and the same is true for its determinant, which must therefore be zero.
Thus, the eigenvalues of $T$ are the solutions to  $\abs{A - \lambda I} = 0$, which is known as the
\textbf{characteristic equation}.

\begin{prop}[Characterization of eigenvalues]
    Given a finite-dimension vector space $V$ over a field \field and an endomorphism $T\in\endospace{V}$,
    the scalar $\lambda\in\field$ is an eigenvalue of $T\iff$ it is a solution to the equation $\abs{A -
        \lambda I} = 0$ where $A$ is the matrix of $T$ in a fixed basis.
\end{prop}

\begin{defn}[Characteristic polynomial]
    Given the matrix $A\in\mathcal{M}_n\left( \field \right) $, the characteristic polynomial $p_A\left(
    \lambda \right) $ of $A$ is defined as $p_A\left( \lambda \right) \bydef \abs{A - \lambda I}$. If
    $T\in\endospace{V}$, $p_T\left( \lambda \right) $ is the characteristic polynomial of a matrix $A$
    associated to the endomorphism.
\end{defn}

\begin{coro}
    Let $V$ be an $n$-dimensional vector space and $T\in\endospace{V}$. Then, its characteristic polynomial
    $p_T
    \left( \lambda \right) $ has degree $n$ and its roots are the eigenvalues of $T$. Moreover, if $p_T\left(
    \lambda\right) $ has exactly $n$ different roots, $\lambda_1, \ldots, \lambda_n$, then the endomorphism
    is diagonalizable in a basis of eigenvectors $\vec{v_1}, \ldots, \vec{v_n}$, where $T\left( \vec{v_i}
    \right) = \lambda_i\vec{v_i}$ for $i = 1, \ldots, n$.
\end{coro}

To properly talk about \textit{characteristic polynomial} is necessary to prove that $\abs{A - \lambda I}$
is independent of the chosen basis of $V$ to write its matrix.

\begin{proof}
    Let $p_\mathcal{B}\left( \lambda \right) = \abs{A - \lambda I}$ be the characteristic polynomial of an
    endomorphism $T$ in the basis $\mathcal{B}$ and let $p_\mathcal{B'} = \abs{A' - \lambda I}$ be the
    characteristic polynomial of $T$ in the basis $\mathcal{B'}$. If $C$ is the change of basis matrix from
    $\mathcal{B}$ to $\mathcal{B'}$ it is known that $A' = \invers{C}AC$ and, therefore,
    \begin{align}
        p_\mathcal{B'}\left( \lambda \right)  &= \abs{A' - \lambda I} = \abs{\invers{C}AC - \lambda I} =
        \abs{\invers{C}AC - \invers{C}\lambda IC} = \\ &= \abs{\invers{C}}\abs{A - \lambda I}\abs{C} =
        \abs{A - \lambda I} = p_\mathcal{B}\left( \lambda \right).
    \end{align}
\end{proof}


%\section{Matrix diagonalization}

\section{Minimal polynomial}

\section{Singular Value Decomposition}

\section{Invariant subspaces. Cayley-Hamilton theorem}

\section{Jordan canonical form of a matrix}
